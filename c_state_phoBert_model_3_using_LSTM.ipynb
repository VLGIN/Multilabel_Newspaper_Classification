{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"c_state_phoBert_model_3_using_LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"VXk7kLt7T84C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1624088548383,"user_tz":-420,"elapsed":5183,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}},"outputId":"a67448ff-988e-48a2-dad0-bdaeffde5115"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","import os\n","os.chdir(\"/content/drive/MyDrive/BERT\")\n","!pip install transformers\n","from transformers import AutoModel, AutoTokenizer\n","import re\n","import pandas as pd\n","import numpy as np\n","import random\n","import torch\n","from torch import nn, optim\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score, accuracy_score\n","from transformers import get_linear_schedule_with_warmup, get_constant_schedule, AdamW\n","\n","torch.cuda.empty_cache()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.7.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.5.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: huggingface-hub==0.0.8 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"g4YHrb8TUn5c"},"source":["# Read data\n","\n","Split train and test set.\n","\n","Encode data usinng tokenizer from pretrained phobert."]},{"cell_type":"code","metadata":{"id":"D8tZpW82UX5b","executionInfo":{"status":"ok","timestamp":1624088548385,"user_tz":-420,"elapsed":14,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["# def read_data(path):\n","#   with open(path) as f:\n","#     data = f.read().splitlines()\n","#   return data"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTDI-HkWU3Jr","executionInfo":{"status":"ok","timestamp":1624088548386,"user_tz":-420,"elapsed":13,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["# data = read_data(\"80k_dataset/data.txt\")\n","# label = read_data(\"80k_dataset/label.txt\")"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Vasc5qu3nPm","executionInfo":{"status":"ok","timestamp":1624088548851,"user_tz":-420,"elapsed":477,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["dataset = pd.read_csv(\"dataset_original.csv\")\n","data = dataset[\"data\"].values\n","label = dataset[\"label\"].values"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRbYHa0u535j","executionInfo":{"status":"ok","timestamp":1624088548852,"user_tz":-420,"elapsed":10,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}},"outputId":"769f3700-5d2a-43ee-8d6b-bb44c0f6e009"},"source":["len(data)"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["135622"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"3Rutcd1vWqdo","executionInfo":{"status":"ok","timestamp":1624088548852,"user_tz":-420,"elapsed":5,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["# def remove_quotes_and_parenthesis(data):\n","#   result = []\n","#   for text in data:\n","#     result.append(re.sub(r\"[\\\"\\']\", \"\", text))\n","#   return result\n","# data = remove_quotes_and_parenthesis(data)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"AX3O9AgYWvG1","executionInfo":{"status":"ok","timestamp":1624088548853,"user_tz":-420,"elapsed":5,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["train_dataset = {\"data\": data[:80000],\"label\": label[:80000]}\n","test_dataset = {\"data\": data[80000:], \"label\": label[80000:]}"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m8N_S88KVYKW","executionInfo":{"status":"ok","timestamp":1624088548853,"user_tz":-420,"elapsed":5,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}},"outputId":"3dcf42cb-f9e4-4833-aee9-22ef66ba591e"},"source":["print(\"Train data:\\n\")\n","for i in range(5):\n","  print(\"text: \", train_dataset[\"data\"][i], \"\\nlabel\", train_dataset[\"label\"][i])\n","print(\"Test data:\\n\")\n","for i in range(5):\n","  print(\"text: \", test_dataset[\"data\"][i], \"\\nlabel\", test_dataset[\"label\"][i])"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Train data:\n","\n","text:   Ảnh : Vũ_Di Tài_xế Nguyễn_Thế_Vũ kể : Khi lên xe , ngay_lập_tức thanh_niên xăm trổ liền khống_chế tôi , thu hết điện_thoại và giấy_tờ xe . \n","label Pháp luật.txt\n","text:   Dù không ở khách_sạn để tránh tốn_kém nhưng ông Park cũng đề_nghị VFF phải duy_trì thực_đơn ăn_uống như tại khách_sạn cao_cấp , nếu các tuyển_thủ muốn ăn gì thì phải để họ được ăn . \n","label Thể thao.txt\n","text:   Ông Lê Nguyễn_Minh_Quang , Trưởng Ban đường_sắt đô_thị TP HCM cho biết : Chúng_tôi vẫn kiên_trì đeo_bám , kiến_nghị làm_việc cụ_thể . \n","label Xã hội.txt\n","text:   * Trong bối_cảnh quan_hệ Thổ_Nhĩ_Kỳ với phương Tây đang xuống_dốc không phanh , thì sự_cố xảy ra tại cuộc tập_trận chung của Liên_minh quân_sự tại Na_Uy vừa_rồi như là một cú đẩy Ankara lại gần hơn_nữa với các đối_thủ của NATO là Nga và Iran NATO đang đẩy Thổ_Nhĩ_Kỳ ngã vào lòng Nga ? \n","label Thế giới.txt\n","text:   Chúng_tôi xin giới_thiệu một_số điểm du_lịch thú_vị được nhiều người chọn nhất hiện_nay để các gia_đình có_thể dễ_dàng lựa_chọn : Săn ảnh miền núi_rừng phía Bắc_Nếu những_ai yêu thích nhiếp_ảnh thì đây là cơ_hội tuyệt_vời để bạn đặt_chân lên miền rẻo_cao phương Bắc , dù Đông_Bắc hùng_vĩ hay Tây_Bắc hoang_sơ đều mang đến bạn thật nhiều cảm_xúc để sáng_tác nên những bức ảnh đẹp . \n","label Văn hóa.txt\n","Test data:\n","\n","text:   Tuy_nhiên , khi SAOstar liên_hệ với phía Nguyễn_Thị_Loan để làm rõ thông_tin , người đại_diện của cô cho_hay , chân dài gốc Thái_Bình chọn giải_pháp im_lặng , bởi tất_cả vẫn là kế_hoạch . \n","label Giải trí.txt\n","text:   Các bộ_trưởng quốc_phòng ASEAN tham_gia khai_mạc Hội_nghị bộ_trưởng quốc_phòng ASEAN ADMM lần thứ ở Philippines ngày . \n","label Thế giới.txt\n","text:   Động_cơ vẫn chưa tiết_lộ EcoSport cũng trang_bị động_cơ tăng áp EcoBoost . \n","label Xe cộ.txt\n","text:   Từ đây , đường_dây buôn_bán phụ_nữ qua biên_giới Trung_Việt , tới các tỉnh Hà_Nam , An_Huy , Giang_Tây bị phơi_bày . \n","label Pháp luật.txt\n","text:   Cách sử_dụng dầu gạo cũng tương_tự như các loại dầu ăn khác , có_thể ăn trực_tiếp như trộn salad hoặc dùng để nấu . \n","label Đời sống.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwxzApC5WmJz","executionInfo":{"status":"ok","timestamp":1624088551161,"user_tz":-420,"elapsed":2312,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}},"outputId":"37380aa4-4ff4-4fe9-f3d3-e080e3ae6867"},"source":["tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n","\n","sequence_length = 128\n","\n","def encode_data(data, tokenizer):\n","  result = []\n","  for text in data:\n","    temp = tokenizer.encode(text)\n","\n","    if(len(temp) < sequence_length):\n","      temp += [1]*(sequence_length-len(temp))\n","    elif len(temp) > sequence_length:\n","      temp = temp[:sequence_length]\n","      temp[-1] = tokenizer.eos_token_id\n","    result.append(temp)\n","  return result"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vb_0ErNMX8X3","executionInfo":{"status":"ok","timestamp":1624088579566,"user_tz":-420,"elapsed":28414,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}},"outputId":"29edf1bd-f48a-4a56-fced-ba48a91ff700"},"source":["train_dataset[\"data\"] = encode_data(train_dataset[\"data\"], tokenizer)\n","test_dataset[\"data\"] = encode_data(test_dataset[\"data\"], tokenizer)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (314 > 256). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aJ5nTv1gY_8c","executionInfo":{"status":"ok","timestamp":1624088579569,"user_tz":-420,"elapsed":32,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["lbencode = LabelEncoder()\n","lbencode.fit(label)\n","train_dataset[\"label\"] = lbencode.transform(train_dataset[\"label\"])\n","test_dataset[\"label\"] = lbencode.transform(test_dataset[\"label\"])"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6XCd5S5Yui0"},"source":["# Transform to torch dataset and dataloader"]},{"cell_type":"code","metadata":{"id":"fRl2ZaNKYoz6","executionInfo":{"status":"ok","timestamp":1624088580181,"user_tz":-420,"elapsed":640,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["train_dataset_torch = torch.utils.data.TensorDataset(torch.tensor(train_dataset[\"data\"], dtype=torch.long), torch.tensor(train_dataset[\"label\"], dtype=torch.long))\n","test_dataset_torch = torch.utils.data.TensorDataset(torch.tensor(test_dataset[\"data\"], dtype = torch.long), torch.tensor(test_dataset[\"label\"], dtype  = torch.long))"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"_G3JzPbBaEeG","executionInfo":{"status":"ok","timestamp":1624088580181,"user_tz":-420,"elapsed":5,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["train_loader = torch.utils.data.DataLoader(train_dataset_torch, batch_size=8, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset_torch, batch_size=8, shuffle=False)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-63VwF8maq_E"},"source":["# Build fine-tuning model to classify data."]},{"cell_type":"code","metadata":{"id":"QB16ODDDapVz","executionInfo":{"status":"ok","timestamp":1624088580182,"user_tz":-420,"elapsed":5,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["class Model(nn.Module):\n","  def __init__(self, n_classes):\n","    super(Model, self).__init__()\n","    self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n","    self.lstm = nn.LSTM(input_size=768, hidden_size=256, batch_first=True)\n","    self.drop = nn.Dropout(p=0.3)\n","    self.linear1 = nn.Linear(256, 128)\n","    self.linear2 = nn.Linear(128, n_classes)\n","    self.softmax = nn.Softmax(dim = 1)\n","  def forward(self, inputs):\n","    cls_embedding = self.phobert(inputs)[0]\n","    lstm_embedding = self.lstm(cls_embedding)[1][1][0]\n","    lstm_embedding = torch.nn.functional.relu(lstm_embedding)\n","    #lstm_embedding = torch.mean(lstm_embedding, dim = 1)\n","    output = self.drop(lstm_embedding)\n","    output = self.linear1(lstm_embedding)\n","    output = torch.nn.functional.relu(output)\n","    output = self.linear2(output)\n","    return self.softmax(output)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vFWz4cqWyu9Z"},"source":["# Declare training, validation and evaluation function"]},{"cell_type":"code","metadata":{"id":"ggQpNRTK0qe5","executionInfo":{"status":"ok","timestamp":1624088580182,"user_tz":-420,"elapsed":5,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["def evaluate(y_pred, y_batch):\n","  with torch.no_grad():\n","    y_pred = y_pred.detach().cpu().numpy()\n","    label = np.argmax(y_pred, axis = 1)\n","    y_batch = y_batch.detach().cpu().numpy()\n","    f1 = f1_score(y_batch, label, average='weighted')\n","    acc = accuracy_score(y_batch, label)\n","  return f1, acc"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"76fbHDvX1U6m","executionInfo":{"status":"ok","timestamp":1624088580183,"user_tz":-420,"elapsed":6,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["def validation(test_loader, loss, model, device):\n","  f1s = []\n","  accs = []\n","  model.eval()\n","  with torch.no_grad():\n","    y_ = []\n","    y = []\n","    y_pred_ = []\n","    for x_batch, y_batch in test_loader:\n","      x_batch = x_batch.to(device)\n","      y.extend(y_batch.numpy())\n","      y_pred = model(x_batch)\n","      y_pred = y_pred.detach().cpu().numpy()\n","      y_pred_.extend(y_pred)\n","      y_pred = np.argmax(y_pred, axis = 1)\n","      y_.extend(y_pred)\n","    f1 = f1_score(y, y_, average = None)\n","    f1s = f1_score(y, y_, average = 'weighted')\n","    accs = accuracy_score(y, y_)\n","    y_pred_ = torch.tensor(y_pred_, dtype= torch.float).to(device)\n","    y = torch.tensor(y, dtype = torch.long).to(device)\n","    loss_value = loss(y_pred_, y)\n","\n","  return loss_value.item(), f1, f1s, accs"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"Twgyx1ec1YXK","executionInfo":{"status":"ok","timestamp":1624088580183,"user_tz":-420,"elapsed":5,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":["def trainOnEpoch(train_loader, model, optimizer, loss, num_epochs, epoch, device, scheduler, history, log_time = 100):\n","  loss_epoch = 0\n","  acc_epoch = 0\n","  f1_epoch = 0\n","  for i, (x_batch, y_batch) in enumerate(train_loader):\n","    model.train()\n","    x_batch = x_batch.to(device)\n","    y_batch = y_batch.to(device)\n","    optimizer.zero_grad()\n","    y_pred = model(x_batch)\n","    loss_value = loss(y_pred, y_batch)\n","    f1, acc = evaluate(y_pred, y_batch)\n","\n","    loss_value.backward()\n","    #nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n","    optimizer.step()\n","    loss_epoch += loss_value.item()\n","    acc_epoch += acc\n","    f1_epoch += f1\n","    scheduler.step()\n","    if (i+1) % log_time == 0 or i + 1 == len(train_loader):\n","      print(\"[TRAIN EPOCH {}] batch {} / {}, loss: {}, acc: {}, f1_avg:{}\".format(epoch+1, i+1, len(train_loader), loss_epoch / (i + 1), acc_epoch / (i+1), f1_epoch / (i + 1)))\n","      history[\"loss_train\"].append(loss_epoch / (i+1))\n","      history[\"acc_train\"].append(acc_epoch / (i+1))\n","      history[\"f1_train\"].append(f1_epoch / (i + 1))"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"895xtowbgglM"},"source":["# Declare hyperparameters and train model."]},{"cell_type":"code","metadata":{"id":"MWa7fvcY8lm8","colab":{"base_uri":"https://localhost:8080/","height":421},"executionInfo":{"status":"error","timestamp":1624089917499,"user_tz":-420,"elapsed":1196,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}},"outputId":"03a4080f-039d-437b-e2d3-2a0e3c9299eb"},"source":["EPOCHS = 50\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","CHECKPOINT_PATH = \"checkpoint_model_3/last_model.pth.tar\"\n","CHECKPOINT_BEST_PATH = \"checkpoint_model_3/best_model.path.tar\"\n","\n","history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}\n","\n","model = Model(11)\n","model.to(DEVICE)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=1e-5)\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_loader)*(EPOCHS-1))\n","scheduler_frozen = get_constant_schedule(optimizer)\n","start_epoch = 0\n","frozen = True\n","for child in model.phobert.children():\n","  for param in child.parameters():\n","    param.requires_grad = False\n","\n","if os.path.exists(\"history_model_3/f1_valid.txt\"):\n","  with open(\"history_model_3/f1_valid.txt\") as f:\n","    max_score = f.read().splitlines()\n","  start_epoch = len(max_score)\n","  max_score = np.array(max_score).astype(float)\n","  last_score = max_score[-1]\n","  max_score = np.max(max_score)\n","else:\n","  max_score = 0\n","  last_score = 0\n","\n","if os.path.exists(CHECKPOINT_PATH):\n","  checkpoint = torch.load(CHECKPOINT_PATH)\n","  model.load_state_dict(checkpoint[\"model_state_dict\"])\n","  optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","  scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","  loss_function.load_state_dict(checkpoint[\"loss_state_dict\"])\n","\n","for epoch in range(start_epoch, EPOCHS, 1):\n","  if epoch > 0 and frozen:\n","    for child in model.phobert.children():\n","      for param in child.parameters():\n","        param.requires_grad = True\n","    del scheduler_frozen\n","    torch.cuda.empty_cache()\n","    frozen = False\n","  print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\")\n","\n","  if epoch == 30:\n","    dict_ = optimizer.state_dict()\n","    dict_['param_groups'][0]['initial_lr'] = scheduler.state_dict()['_last_lr']\n","    dict_['param_groups'][0]['lr'] = scheduler.state_dict()['_last_lr']\n","    optimizer.load_state_dict(dict_)\n","    scheduler = get_constant_schedule(optimizer)\n","\n","  if frozen:\n","    trainOnEpoch(train_loader=train_loader, model = model, optimizer = optimizer, loss= loss_function, num_epochs = EPOCHS, epoch = epoch, device = DEVICE, scheduler = scheduler_frozen, history=history)\n","  else:\n","    trainOnEpoch(train_loader=train_loader, model = model, optimizer = optimizer, loss= loss_function, num_epochs = EPOCHS, epoch = epoch,device = DEVICE, scheduler = scheduler, history= history)\n","  losses, f1_classes, f1, acc = validation(test_loader, loss_function, model, DEVICE)\n","  \n","  print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\", \" acc: {}, f1_score: {}\".format(acc, f1))\n","\n","  with open('history_model_3/loss.txt', 'a+') as f:\n","    f.write(str(losses) + \"\\n\")\n","  with open('history_model_3/f1_classes.txt', 'a+') as f:\n","    f.write(' '.join(f1_classes.astype(str)) + \"\\n\")\n","  with open(\"history_model_3/loss_train.txt\", \"a+\") as f:\n","    for item in history[\"loss_train\"]:\n","      f.write(str(item) + \"\\n\")\n","  with open(\"history_model_3/acc_train.txt\", \"a+\") as f:\n","    for item in history[\"acc_train\"]:\n","      f.write(str(item) + \"\\n\")\n","  with open(\"history_model_3/f1_train.txt\", \"a+\") as f:\n","    for item in history[\"f1_train\"]:\n","      f.write(str(item) + \"\\n\")\n","  with open(\"history_model_3/acc_valid.txt\", \"a+\") as f:\n","    f.write(str(acc) + \"\\n\")\n","  with open(\"history_model_3/f1_valid.txt\", \"a+\") as f:\n","    f.write(str(f1) + \"\\n\")\n","  dict_ = {\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        \"scheduler_state_dict\": scheduler.state_dict(),\n","        \"loss_state_dict\": loss_function.state_dict(),\n","  }\n","  torch.save(dict_, CHECKPOINT_PATH)\n","  #Save model chechk point\n","  if f1 > max_score:\n","    torch.save(dict_, CHECKPOINT_BEST_PATH)\n","    max_score = f1\n","  elif f1 - last_score < -0.02:\n","    break\n","  last_score = f1\n","  history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}"],"execution_count":18,"outputs":[{"output_type":"stream","text":["[TRAIN EPOCH 2] batch 900 / 10000, loss: 2.3266142263677385, acc: 0.21513888888888888, f1_avg:0.10630875220458466\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-ec2950d31675>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtrainOnEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler_frozen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mtrainOnEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-5e055a0ca443>\u001b[0m in \u001b[0;36mtrainOnEpoch\u001b[0;34m(train_loader, model, optimizer, loss, num_epochs, epoch, device, scheduler, history, log_time)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    214\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    983\u001b[0m                           \u001b[0;34m\"non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                           \"github.com/pytorch/pytorch/pull/30531 for more information.\", stacklevel=2)\n\u001b[0;32m--> 985\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"n-n3Os-S0zWz","executionInfo":{"status":"aborted","timestamp":1624089917497,"user_tz":-420,"elapsed":16,"user":{"displayName":"L E O","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibM4NA2VbGTWFha_Ti7MFrmo7DrIfmTAhu4sxd=s64","userId":"15425506064492046936"}}},"source":[""],"execution_count":null,"outputs":[]}]}