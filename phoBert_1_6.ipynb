{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"phoBert_1_6.ipynb","provenance":[],"collapsed_sections":["-63VwF8maq_E"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4968f124430749cd8be3f3fc918b41d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_95b849d552dc4c9dab3580fd9cded732","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_8115ccea20204ba281fc665249c10b0d","IPY_MODEL_2c4545389adf4e7e9163507d17ad02bc"]}},"95b849d552dc4c9dab3580fd9cded732":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8115ccea20204ba281fc665249c10b0d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_4a01abd641804b098b0aeec611da8e82","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":542923308,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":542923308,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_947d64fe031946dfabd95e5acd25db03"}},"2c4545389adf4e7e9163507d17ad02bc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e3184fcfd87a4fe69d88b0ee9804d6a4","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 543M/543M [00:11&lt;00:00, 47.0MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9ea77619e94f49a8a85a283695694d15"}},"4a01abd641804b098b0aeec611da8e82":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"947d64fe031946dfabd95e5acd25db03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e3184fcfd87a4fe69d88b0ee9804d6a4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9ea77619e94f49a8a85a283695694d15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"VXk7kLt7T84C"},"source":["from google.colab import drive\n","drive.mount(\"/content/drive\")\n","import os\n","os.chdir(\"/content/drive/MyDrive/BERT\")\n","!pip install transformers\n","from transformers import AutoModel, AutoTokenizer\n","import re\n","import numpy as np\n","import random\n","import torch\n","from torch import nn, optim\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import f1_score, accuracy_score\n","from transformers import get_linear_schedule_with_warmup, get_constant_schedule, AdamW"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g4YHrb8TUn5c"},"source":["# Read data\n","\n","Split train and test set.\n","\n","Encode data usinng tokenizer from pretrained phobert."]},{"cell_type":"code","metadata":{"id":"D8tZpW82UX5b"},"source":["def read_data(path):\n","  with open(path) as f:\n","    data = f.read().splitlines()\n","  return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zTDI-HkWU3Jr"},"source":["data = read_data(\"80k_dataset/data.txt\")\n","label = read_data(\"80k_dataset/label.txt\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Rutcd1vWqdo"},"source":["def remove_quotes_and_parenthesis(data):\n","  result = []\n","  for text in data:\n","    result.append(re.sub(r\"[\\\"\\']\", \"\", text))\n","  return result\n","data = remove_quotes_and_parenthesis(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AX3O9AgYWvG1"},"source":["train_dataset = {\"data\": data[:60000],\"label\": label[:60000]}\n","test_dataset = {\"data\": data[60000:], \"label\": label[60000:]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m8N_S88KVYKW","executionInfo":{"status":"ok","timestamp":1623078181138,"user_tz":-420,"elapsed":12,"user":{"displayName":"Giang Vũ Long","photoUrl":"","userId":"12078697681841975270"}},"outputId":"3911db6d-1d3b-495c-d14d-97bc1f17aa8c"},"source":["print(\"Train data:\\n\")\n","for i in range(5):\n","  print(\"text: \", train_dataset[\"data\"][i], \"\\nlabel\", train_dataset[\"label\"][i])\n","print(\"Test data:\\n\")\n","for i in range(5):\n","  print(\"text: \", test_dataset[\"data\"][i], \"\\nlabel\", test_dataset[\"label\"][i])"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Train data:\n","\n","text:   Các nhà_khoa_học đã phát_hiện hoá_thạch của một con rùa nước_ngọt châu_Á tại Bắc_cực , và đặt tên là auroral . \n","label Khoa học.txt\n","text:   Trong ngày thi_đấu , chỉ có cặp đấu được diễn ra , song đó lại là những màn so tài rất đáng để chờ_đợi . \n","label Thể thao.txt\n","text:   Theo hãng tin Hàn_Quốc Yonhap , tính đến tháng , sau chưa đầy một tháng ra_mắt , lượng đặt_hàng trước của Galaxy_SS + tăng gấp lần so với Galaxy_SS edge . \n","label Công nghệ.txt\n","text:   Mẫu_SUV cỡ nhỏ thiết_kế thể_thao , sử_dụng động_cơ , lít TFSI công_suất mã_lực , hướng đến người Việt trẻ . \n","label Xe cộ.txt\n","text:   Quảng_cáo trên xe cửa_ô tô cá_nhân là hình_thức mới trong phân khúc quảng_cáo ngoài_trời , đã thịnh_hành trên thế_giới và được ví là mô_hình Uber for ads chia_sẻ quảng_cáo . \n","label None.txt\n","Test data:\n","\n","text:   Tôi là người không giỏi chịu_đựng , buồn một_chút thôi là suy_nghĩ tiêu_cực nhưng tôi vẫn tìm lại anh với suy_nghĩ mình cứ bên cạnh họ , trân thành họ thì chắc_chắn họ cũng sẽ hiểu ra mà thay_đổi . \n","label Pháp luật.txt\n","text:   Trong khi đó , các cơ_sở đào_tạo công_lập chưa tự_chủ tài_chính cần phải dựa vào bầu sữa của ngân_sách . \n","label Giáo dục.txt\n","text:   Tổng_thống Venezuela Nicolas_Maduro ngoài cùng bên trái kêu_gọi quân_đội sẵn_sàng bảo_vệ đất_nước trước đe_doạ từ Mỹ . \n","label Thế giới.txt\n","text:   Chiều ngày , thông_tin từ Sở Tài_nguyên và Môi_trường Thanh_Hoá cho biết đã hoàn_thành việc chôn lấp toàn_bộ số lợn bị chết tại khu_vực Trại_giam số Yên_Định , Thanh_Hoá . \n","label Xã hội.txt\n","text:   Các trạm quan_trắc môi_trường nước do Trung_tâm vi_mạch Đà_Nẵng sản_xuất Ngày . \n","label Công nghệ.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UwxzApC5WmJz"},"source":["tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\")\n","\n","def encode_data(data, tokenizer):\n","  result = []\n","  for text in data:\n","    temp = tokenizer.encode(text)\n","\n","    if(len(temp) < 64):\n","      temp += [1]*(64-len(temp))\n","    elif len(temp) > 64:\n","      temp = temp[:64]\n","      temp[-1] = tokenizer.eos_token_id\n","    result.append(temp)\n","  return result"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vb_0ErNMX8X3","executionInfo":{"status":"ok","timestamp":1623078202116,"user_tz":-420,"elapsed":17293,"user":{"displayName":"Giang Vũ Long","photoUrl":"","userId":"12078697681841975270"}},"outputId":"536d966e-304f-42cc-f1f1-612d0bb667a3"},"source":["train_dataset[\"data\"] = encode_data(train_dataset[\"data\"], tokenizer)\n","test_dataset[\"data\"] = encode_data(test_dataset[\"data\"], tokenizer)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (302 > 256). Running this sequence through the model will result in indexing errors\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"aJ5nTv1gY_8c"},"source":["lbencode = LabelEncoder()\n","lbencode.fit(label)\n","train_dataset[\"label\"] = lbencode.transform(train_dataset[\"label\"])\n","test_dataset[\"label\"] = lbencode.transform(test_dataset[\"label\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n6XCd5S5Yui0"},"source":["# Transform to torch dataset and dataloader"]},{"cell_type":"code","metadata":{"id":"fRl2ZaNKYoz6"},"source":["train_dataset_torch = torch.utils.data.TensorDataset(torch.tensor(train_dataset[\"data\"], dtype=torch.long), torch.tensor(train_dataset[\"label\"], dtype=torch.long))\n","test_dataset_torch = torch.utils.data.TensorDataset(torch.tensor(test_dataset[\"data\"], dtype = torch.long), torch.tensor(test_dataset[\"label\"], dtype  = torch.long))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_G3JzPbBaEeG"},"source":["train_loader = torch.utils.data.DataLoader(train_dataset_torch, batch_size=8, shuffle=False)\n","test_loader = torch.utils.data.DataLoader(test_dataset_torch, batch_size=8, shuffle=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-63VwF8maq_E"},"source":["# Build fine-tuning model to classify data."]},{"cell_type":"code","metadata":{"id":"QB16ODDDapVz"},"source":["class Model(nn.Module):\n","  def __init__(self, n_classes):\n","    super(Model, self).__init__()\n","    self.phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n","    self.drop = nn.Dropout(p=0.3)\n","    self.linear1 = nn.Linear(768, 256)\n","    self.linear2 = nn.Linear(256, n_classes)\n","    self.softmax = nn.Softmax(dim = 1)\n","  def forward(self, inputs):\n","    cls_embedding = self.phobert(inputs)[0][:, 0,:]\n","    output = self.drop(cls_embedding)\n","    output = self.linear1(output)\n","    output = torch.nn.functional.relu(output)\n","    output = self.linear2(output)\n","    return self.softmax(output)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vFWz4cqWyu9Z"},"source":["# Declare training, validation and evaluation function"]},{"cell_type":"code","metadata":{"id":"ggQpNRTK0qe5"},"source":["def evaluate(y_pred, y_batch):\n","  with torch.no_grad():\n","    y_pred = y_pred.detach().cpu().numpy()\n","    label = np.argmax(y_pred, axis = 1)\n","    y_batch = y_batch.detach().cpu().numpy()\n","    f1 = f1_score(y_batch, label, average='weighted')\n","    acc = accuracy_score(y_batch, label)\n","  return f1, acc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"76fbHDvX1U6m"},"source":["def validation(test_loader, model, device):\n","  f1s = []\n","  accs = []\n","  model.eval()\n","  with torch.no_grad():\n","    for x_batch, y_batch in test_loader:\n","      x_batch = x_batch.to(device)\n","      y_batch = y_batch.to(device)\n","      y_pred = model(x_batch)\n","      f1, acc = evaluate(y_pred, y_batch)\n","      f1s.append(f1)\n","      accs.append(acc)\n","    f1s = np.mean(f1s)\n","    accs = np.mean(accs)\n","\n","  return f1s, accs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Twgyx1ec1YXK"},"source":["def trainOnEpoch(train_loader, model, optimizer, loss, num_epochs, epoch, device, scheduler, history, log_time = 100):\n","  loss_epoch = 0\n","  acc_epoch = 0\n","  f1_epoch = 0\n","  for i, (x_batch, y_batch) in enumerate(train_loader):\n","    model.train()\n","    x_batch = x_batch.to(device)\n","    y_batch = y_batch.to(device)\n","    optimizer.zero_grad()\n","    y_pred = model(x_batch)\n","    loss_value = loss(y_pred, y_batch)\n","    acc, f1 = evaluate(y_pred, y_batch)\n","\n","    loss_value.backward()\n","    #nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n","    optimizer.step()\n","    loss_epoch += loss_value.item()\n","    acc_epoch += acc\n","    f1_epoch += f1\n","    scheduler.step()\n","    if (i+1) % log_time == 0:\n","      print(\"[TRAIN EPOCH {}] batch {} / {}, loss: {}, acc: {}, f1_avg:{}\".format(epoch+1, i+1, len(train_loader), loss_epoch / (i + 1), acc_epoch / (i+1), f1_epoch / (i + 1)))\n","      history[\"loss_train\"].append(loss_epoch / (i+1))\n","      history[\"acc_train\"].append(acc_epoch / (i+1))\n","      history[\"f1_train\"].append(f1_epoch / (i + 1))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"895xtowbgglM"},"source":["# Declare hyperparameters and train model."]},{"cell_type":"code","metadata":{"id":"MWa7fvcY8lm8"},"source":["# EPOCHS = 20\n","# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# CHECKPOINT_PATH = \"checkpoint/last_model.pth.tar\"\n","\n","# history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}\n","\n","# model = Model(14)\n","# model.to(DEVICE)\n","# loss_function = nn.CrossEntropyLoss()\n","# optimizer = AdamW(model.parameters(), lr=2e-5)\n","# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=len(train_loader)*(EPOCHS-1))\n","# scheduler_frozen = get_constant_schedule(optimizer)\n","# start_epoch = 0\n","# frozen = True\n","# for child in model.phobert.children():\n","#   for param in child.parameters():\n","#     param.requires_grad = False\n","\n","# if os.path.exists(\"history/f1_valid.txt\"):\n","#   with open(\"history/f1_valid.txt\") as f:\n","#     max_score = f.read().splitlines()\n","#   start_epoch = len(max_score)\n","#   max_score = np.array(max_score).astype(float)\n","#   max_score = np.max(max_score)\n","# else:\n","#   max_score = 0\n","\n","# if os.path.exists(CHECKPOINT_PATH):\n","#   checkpoint = torch.load(CHECKPOINT_PATH)\n","#   model.load_state_dict(checkpoint[\"model_state_dict\"])\n","#   optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","#   scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","#   loss_function.load_state_dict(checkpoint[\"loss_state_dict\"])\n","\n","# for epoch in range(start_epoch, EPOCHS, 1):\n","#   if epoch > 0 and frozen:\n","#     for child in model.phobert.children():\n","#       for param in child.parameters():\n","#         param.requires_grad = True\n","#     del scheduler_frozen\n","#     torch.cuda.empty_cache()\n","#     frozen = False\n","#   print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\")\n","#   if frozen:\n","#     trainOnEpoch(train_loader=train_loader, model = model, optimizer = optimizer, loss= loss_function, num_epochs = EPOCHS, epoch = epoch, device = DEVICE, scheduler = scheduler_frozen, history=history)\n","#   else:\n","#     trainOnEpoch(train_loader=train_loader, model = model, optimizer = optimizer, loss= loss_function, num_epochs = EPOCHS, epoch = epoch,device = DEVICE, scheduler = scheduler, history= history)\n","#   acc, f1 = validation(test_loader, model, DEVICE)\n","  \n","#   print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\", \" acc: {}, f1_score: {}\".format(acc, f1))\n","\n","#   with open(\"history/loss_train.txt\", \"a+\") as f:\n","#     for item in history[\"loss_train\"]:\n","#       f.write(str(item) + \"\\n\")\n","#   with open(\"history/acc_train.txt\", \"a+\") as f:\n","#     for item in history[\"acc_train\"]:\n","#       f.write(str(item) + \"\\n\")\n","#   with open(\"history/f1_train.txt\", \"a+\") as f:\n","#     for item in history[\"f1_train\"]:\n","#       f.write(str(item) + \"\\n\")\n","#   with open(\"history/acc_valid.txt\", \"a+\") as f:\n","#     f.write(str(acc) + \"\\n\")\n","#   with open(\"history/f1_valid.txt\", \"a+\") as f:\n","#     f.write(str(f1) + \"\\n\")\n","\n","#   #Save model chechk point\n","#   if f1 > max_score:\n","#     dict_ = {\n","#         \"model_state_dict\": model.state_dict(),\n","#         \"optimizer_state_dict\": optimizer.state_dict(),\n","#         \"scheduler_state_dict\": scheduler.state_dict(),\n","#         \"loss_state_dict\": loss_function.state_dict(),\n","#     }\n","\n","#     torch.save(dict_, CHECKPOINT_PATH)\n","#     max_score = f1\n","#   history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aT383KidOIt2"},"source":["# EPOCHS = 30\n","# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# CHECKPOINT_PATH = \"checkpoint/last_model.pth.tar\"\n","# CHECKPOINT_BEST_PATH = \"checkpoint/best_model.pth.tar\"\n","\n","# history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}\n","\n","# model = Model(14)\n","# model.to(DEVICE)\n","# loss_function = nn.CrossEntropyLoss()\n","# optimizer = AdamW(model.parameters(), lr=5e-7)\n","# scheduler = get_constant_schedule(optimizer)\n","# start_epoch = 0\n","# frozen = True\n","# for child in model.phobert.children():\n","#   for param in child.parameters():\n","#     param.requires_grad = False\n","\n","# if os.path.exists(\"history/f1_valid.txt\"):\n","#   with open(\"history/f1_valid.txt\") as f:\n","#     max_score = f.read().splitlines()\n","#   start_epoch = len(max_score)\n","#   max_score = np.array(max_score).astype(float)\n","#   max_score = np.max(max_score)\n","# else:\n","#   max_score = 0\n","\n","# if os.path.exists(CHECKPOINT_PATH):\n","#   checkpoint = torch.load(CHECKPOINT_PATH)\n","#   model.load_state_dict(checkpoint[\"model_state_dict\"])\n","#   optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","#   if start_epoch != 20:\n","#     scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","#   loss_function.load_state_dict(checkpoint[\"loss_state_dict\"])\n","\n","# for epoch in range(start_epoch, EPOCHS, 1):\n","#   if epoch > 0 and frozen:\n","#     for child in model.phobert.children():\n","#       for param in child.parameters():\n","#         param.requires_grad = True\n","#     torch.cuda.empty_cache()\n","#     frozen = False\n","#   print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\")\n","#   trainOnEpoch(train_loader=train_loader, model = model, optimizer = optimizer, loss= loss_function, num_epochs = EPOCHS, epoch = epoch,device = DEVICE, scheduler = scheduler, history= history)\n","#   acc, f1 = validation(test_loader, model, DEVICE)\n","  \n","#   print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\", \" acc: {}, f1_score: {}\".format(acc, f1))\n","\n","#   with open(\"history/loss_train.txt\", \"a+\") as f:\n","#     for item in history[\"loss_train\"]:\n","#       f.write(str(item) + \"\\n\")\n","#   with open(\"history/acc_train.txt\", \"a+\") as f:\n","#     for item in history[\"acc_train\"]:\n","#       f.write(str(item) + \"\\n\")\n","#   with open(\"history/f1_train.txt\", \"a+\") as f:\n","#     for item in history[\"f1_train\"]:\n","#       f.write(str(item) + \"\\n\")\n","#   with open(\"history/acc_valid.txt\", \"a+\") as f:\n","#     f.write(str(acc) + \"\\n\")\n","#   with open(\"history/f1_valid.txt\", \"a+\") as f:\n","#     f.write(str(f1) + \"\\n\")\n","\n","#   #Save model chechk point\n","#   dict_ = {\n","#     \"model_state_dict\": model.state_dict(),\n","#     \"optimizer_state_dict\": optimizer.state_dict(),\n","#     \"scheduler_state_dict\": scheduler.state_dict(),\n","#     \"loss_state_dict\": loss_function.state_dict(),\n","#   }\n","\n","#   torch.save(dict_, CHECKPOINT_PATH)\n","\n","#   if f1 > max_score:\n","#     torch.save(dict_, CHECKPOINT_BEST_PATH)\n","#     max_score = f1\n","#   history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x0VcfcVNg6qW"},"source":["# EPOCHS = 40\n","# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# CHECKPOINT_PATH = \"checkpoint/last_model.pth.tar\"\n","# CHECKPOINT_BEST_PATH = \"checkpoint/best_model.pth.tar\"\n","\n","# history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}\n","\n","# model = Model(14)\n","# model.to(DEVICE)\n","# loss_function = nn.CrossEntropyLoss()\n","# optimizer = AdamW(model.parameters(), lr=5e-7)\n","# scheduler = get_linear_schedule_with_warmup(optimizer, 40, num_training_steps=10 * len(train_loader))\n","# start_epoch = 0\n","# frozen = True\n","# for child in model.phobert.children():\n","#   for param in child.parameters():\n","#     param.requires_grad = False\n","\n","# if os.path.exists(\"history/f1_valid.txt\"):\n","#   with open(\"history/f1_valid.txt\") as f:\n","#     max_score = f.read().splitlines()\n","#   start_epoch = len(max_score)\n","#   max_score = np.array(max_score).astype(float)\n","#   max_score = np.max(max_score)\n","# else:\n","#   max_score = 0\n","\n","# if os.path.exists(CHECKPOINT_PATH):\n","#   checkpoint = torch.load(CHECKPOINT_PATH)\n","#   model.load_state_dict(checkpoint[\"model_state_dict\"])\n","#   optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","#   if start_epoch != 30:\n","#     scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","#   loss_function.load_state_dict(checkpoint[\"loss_state_dict\"])\n","\n","# for epoch in range(start_epoch, EPOCHS, 1):\n","#   if epoch > 0 and frozen:\n","#     for child in model.phobert.children():\n","#       for param in child.parameters():\n","#         param.requires_grad = True\n","#     torch.cuda.empty_cache()\n","#     frozen = False\n","#   print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\")\n","#   trainOnEpoch(train_loader=train_loader, model = model, optimizer = optimizer, loss= loss_function, num_epochs = EPOCHS, epoch = epoch,device = DEVICE, scheduler = scheduler, history= history)\n","#   acc, f1 = validation(test_loader, model, DEVICE)\n","  \n","#   print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\", \" acc: {}, f1_score: {}\".format(acc, f1))\n","\n","#   with open(\"history/loss_train.txt\", \"a+\") as f:\n","#     for item in history[\"loss_train\"]:\n","#       f.write(str(item) + \"\\n\")\n","#   with open(\"history/acc_train.txt\", \"a+\") as f:\n","#     for item in history[\"acc_train\"]:\n","#       f.write(str(item) + \"\\n\")\n","#   with open(\"history/f1_train.txt\", \"a+\") as f:\n","#     for item in history[\"f1_train\"]:\n","#       f.write(str(item) + \"\\n\")\n","#   with open(\"history/acc_valid.txt\", \"a+\") as f:\n","#     f.write(str(acc) + \"\\n\")\n","#   with open(\"history/f1_valid.txt\", \"a+\") as f:\n","#     f.write(str(f1) + \"\\n\")\n","\n","#   #Save model chechk point\n","#   dict_ = {\n","#     \"model_state_dict\": model.state_dict(),\n","#     \"optimizer_state_dict\": optimizer.state_dict(),\n","#     \"scheduler_state_dict\": scheduler.state_dict(),\n","#     \"loss_state_dict\": loss_function.state_dict(),\n","#   }\n","\n","#   torch.save(dict_, CHECKPOINT_PATH)\n","\n","#   if f1 > max_score:\n","#     torch.save(dict_, CHECKPOINT_BEST_PATH)\n","#     max_score = f1\n","#   history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["4968f124430749cd8be3f3fc918b41d3","95b849d552dc4c9dab3580fd9cded732","8115ccea20204ba281fc665249c10b0d","2c4545389adf4e7e9163507d17ad02bc","4a01abd641804b098b0aeec611da8e82","947d64fe031946dfabd95e5acd25db03","e3184fcfd87a4fe69d88b0ee9804d6a4","9ea77619e94f49a8a85a283695694d15"]},"id":"cYYeQqvjQGEt","executionInfo":{"status":"ok","timestamp":1623081836029,"user_tz":-420,"elapsed":3633915,"user":{"displayName":"Giang Vũ Long","photoUrl":"","userId":"12078697681841975270"}},"outputId":"625a7353-6c7e-4317-ea31-35d048c85e72"},"source":["EPOCHS = 60\n","DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","CHECKPOINT_PATH = \"checkpoint/last_model.pth.tar\"\n","CHECKPOINT_BEST_PATH = \"checkpoint/best_model.pth.tar\"\n","\n","history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}\n","\n","model = Model(14)\n","model.to(DEVICE)\n","loss_function = nn.CrossEntropyLoss()\n","optimizer = AdamW(model.parameters(), lr=5e-7)\n","scheduler = get_linear_schedule_with_warmup(optimizer, 40, num_training_steps=20 * len(train_loader))\n","start_epoch = 0\n","frozen = True\n","for child in model.phobert.children():\n","  for param in child.parameters():\n","    param.requires_grad = False\n","\n","if os.path.exists(\"history/f1_valid.txt\"):\n","  with open(\"history/f1_valid.txt\") as f:\n","    max_score = f.read().splitlines()\n","  start_epoch = len(max_score)\n","  max_score = np.array(max_score).astype(float)\n","  max_score = np.max(max_score)\n","else:\n","  max_score = 0\n","\n","if os.path.exists(CHECKPOINT_PATH):\n","  checkpoint = torch.load(CHECKPOINT_PATH)\n","  model.load_state_dict(checkpoint[\"model_state_dict\"])\n","  optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","  if start_epoch != 40:\n","    scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n","  loss_function.load_state_dict(checkpoint[\"loss_state_dict\"])\n","\n","for epoch in range(start_epoch, EPOCHS, 1):\n","  if epoch > 0 and frozen:\n","    for child in model.phobert.children():\n","      for param in child.parameters():\n","        param.requires_grad = True\n","    torch.cuda.empty_cache()\n","    frozen = False\n","  print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\")\n","  trainOnEpoch(train_loader=train_loader, model = model, optimizer = optimizer, loss= loss_function, num_epochs = EPOCHS, epoch = epoch,device = DEVICE, scheduler = scheduler, history= history)\n","  acc, f1 = validation(test_loader, model, DEVICE)\n","  \n","  print(\"EPOCH \", epoch+1, \"/\", EPOCHS, \":\", \" acc: {}, f1_score: {}\".format(acc, f1))\n","\n","  with open(\"history/loss_train.txt\", \"a+\") as f:\n","    for item in history[\"loss_train\"]:\n","      f.write(str(item) + \"\\n\")\n","  with open(\"history/acc_train.txt\", \"a+\") as f:\n","    for item in history[\"acc_train\"]:\n","      f.write(str(item) + \"\\n\")\n","  with open(\"history/f1_train.txt\", \"a+\") as f:\n","    for item in history[\"f1_train\"]:\n","      f.write(str(item) + \"\\n\")\n","  with open(\"history/acc_valid.txt\", \"a+\") as f:\n","    f.write(str(acc) + \"\\n\")\n","  with open(\"history/f1_valid.txt\", \"a+\") as f:\n","    f.write(str(f1) + \"\\n\")\n","\n","  #Save model chechk point\n","  dict_ = {\n","    \"model_state_dict\": model.state_dict(),\n","    \"optimizer_state_dict\": optimizer.state_dict(),\n","    \"scheduler_state_dict\": scheduler.state_dict(),\n","    \"loss_state_dict\": loss_function.state_dict(),\n","  }\n","\n","  torch.save(dict_, CHECKPOINT_PATH)\n","\n","  if f1 > max_score:\n","    torch.save(dict_, CHECKPOINT_BEST_PATH)\n","    max_score = f1\n","  history = {\"loss_train\": [], \"acc_train\":[], \"f1_train\":[]}"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4968f124430749cd8be3f3fc918b41d3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=542923308.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"},{"output_type":"stream","text":["EPOCH  58 / 60 :\n","[TRAIN EPOCH 58] batch 100 / 7500, loss: 1.985815590620041, acc: 0.762327380952381, f1_avg:0.7675\n","[TRAIN EPOCH 58] batch 200 / 7500, loss: 1.988469632267952, acc: 0.7605595238095236, f1_avg:0.765625\n","[TRAIN EPOCH 58] batch 300 / 7500, loss: 1.972511569261551, acc: 0.7744742063492063, f1_avg:0.7816666666666666\n","[TRAIN EPOCH 58] batch 400 / 7500, loss: 1.9719489294290542, acc: 0.7744717261904758, f1_avg:0.7825\n","[TRAIN EPOCH 58] batch 500 / 7500, loss: 1.976701859474182, acc: 0.7703690476190476, f1_avg:0.77775\n","[TRAIN EPOCH 58] batch 600 / 7500, loss: 1.9755754421154659, acc: 0.7705545634920632, f1_avg:0.7789583333333333\n","[TRAIN EPOCH 58] batch 700 / 7500, loss: 1.975051783663886, acc: 0.7698163265306117, f1_avg:0.7794642857142857\n","[TRAIN EPOCH 58] batch 800 / 7500, loss: 1.9712630881369113, acc: 0.7740796130952377, f1_avg:0.78328125\n","[TRAIN EPOCH 58] batch 900 / 7500, loss: 1.973068020078871, acc: 0.772199074074074, f1_avg:0.7815277777777778\n","[TRAIN EPOCH 58] batch 1000 / 7500, loss: 1.9722462340593339, acc: 0.7732880952380952, f1_avg:0.782375\n","[TRAIN EPOCH 58] batch 1100 / 7500, loss: 1.9717103735967116, acc: 0.773675865800866, f1_avg:0.7829545454545455\n","[TRAIN EPOCH 58] batch 1200 / 7500, loss: 1.9708152744174003, acc: 0.7742480158730165, f1_avg:0.7838541666666666\n","[TRAIN EPOCH 58] batch 1300 / 7500, loss: 1.9713046360932864, acc: 0.773668498168499, f1_avg:0.7833653846153846\n","[TRAIN EPOCH 58] batch 1400 / 7500, loss: 1.9704652002879552, acc: 0.7743286564625855, f1_avg:0.7841964285714286\n","[TRAIN EPOCH 58] batch 1500 / 7500, loss: 1.971888024409612, acc: 0.7727873015873018, f1_avg:0.7826666666666666\n","[TRAIN EPOCH 58] batch 1600 / 7500, loss: 1.972064518481493, acc: 0.7723214285714286, f1_avg:0.7825\n","[TRAIN EPOCH 58] batch 1700 / 7500, loss: 1.9725433448482963, acc: 0.7717012138188607, f1_avg:0.7819852941176471\n","[TRAIN EPOCH 58] batch 1800 / 7500, loss: 1.9722688913345336, acc: 0.7720074955908283, f1_avg:0.7822222222222223\n","[TRAIN EPOCH 58] batch 1900 / 7500, loss: 1.9722347407591971, acc: 0.7723311925647448, f1_avg:0.7823026315789474\n","[TRAIN EPOCH 58] batch 2000 / 7500, loss: 1.9716125963926314, acc: 0.7729604662698407, f1_avg:0.7829375\n","[TRAIN EPOCH 58] batch 2100 / 7500, loss: 1.9732597402731578, acc: 0.7714448696145116, f1_avg:0.7813095238095238\n","[TRAIN EPOCH 58] batch 2200 / 7500, loss: 1.9748724735325034, acc: 0.769762040043289, f1_avg:0.779715909090909\n","[TRAIN EPOCH 58] batch 2300 / 7500, loss: 1.9755211468364882, acc: 0.7690769927536222, f1_avg:0.7790760869565218\n","[TRAIN EPOCH 58] batch 2400 / 7500, loss: 1.975185275922219, acc: 0.7697320188492053, f1_avg:0.7794270833333333\n","[TRAIN EPOCH 58] batch 2500 / 7500, loss: 1.9746413731575012, acc: 0.770292976190475, f1_avg:0.78\n","[TRAIN EPOCH 58] batch 2600 / 7500, loss: 1.9756537248079593, acc: 0.769353823260072, f1_avg:0.7790384615384616\n","[TRAIN EPOCH 58] batch 2700 / 7500, loss: 1.9756627746864601, acc: 0.7693486552028213, f1_avg:0.7790277777777778\n","[TRAIN EPOCH 58] batch 2800 / 7500, loss: 1.975565444103309, acc: 0.769459289965987, f1_avg:0.7791071428571429\n","[TRAIN EPOCH 58] batch 2900 / 7500, loss: 1.9764611721038818, acc: 0.7684862821565417, f1_avg:0.7782327586206896\n","[TRAIN EPOCH 58] batch 3000 / 7500, loss: 1.976332681020101, acc: 0.7685776124338632, f1_avg:0.778375\n","[TRAIN EPOCH 58] batch 3100 / 7500, loss: 1.976117616584224, acc: 0.7687193100358436, f1_avg:0.7786290322580646\n","[TRAIN EPOCH 58] batch 3200 / 7500, loss: 1.976304481960833, acc: 0.768545789930557, f1_avg:0.7784375\n","[TRAIN EPOCH 58] batch 3300 / 7500, loss: 1.9763479043136944, acc: 0.7684662999038017, f1_avg:0.7784090909090909\n","[TRAIN EPOCH 58] batch 3400 / 7500, loss: 1.9767911696434022, acc: 0.7680890602453118, f1_avg:0.7779411764705882\n","[TRAIN EPOCH 58] batch 3500 / 7500, loss: 1.9772587145056044, acc: 0.7678186585240174, f1_avg:0.7775714285714286\n","[TRAIN EPOCH 58] batch 3600 / 7500, loss: 1.9771479272180132, acc: 0.7678794166867104, f1_avg:0.7777083333333333\n","[TRAIN EPOCH 58] batch 3700 / 7500, loss: 1.9774010003257443, acc: 0.7676825469950496, f1_avg:0.7774662162162163\n","[TRAIN EPOCH 58] batch 3800 / 7500, loss: 1.9775252565898394, acc: 0.7676119536530748, f1_avg:0.7773355263157895\n","[TRAIN EPOCH 58] batch 3900 / 7500, loss: 1.977221998159702, acc: 0.7678976972102, f1_avg:0.7776282051282051\n","[TRAIN EPOCH 58] batch 4000 / 7500, loss: 1.9768992711901665, acc: 0.7683576952561352, f1_avg:0.77796875\n","[TRAIN EPOCH 58] batch 4100 / 7500, loss: 1.9769802569470754, acc: 0.7682271649421059, f1_avg:0.7778963414634147\n","[TRAIN EPOCH 58] batch 4200 / 7500, loss: 1.977185033701715, acc: 0.768024004930257, f1_avg:0.7776785714285714\n","[TRAIN EPOCH 58] batch 4300 / 7500, loss: 1.9779630128727403, acc: 0.7672099084700852, f1_avg:0.7768895348837209\n","[TRAIN EPOCH 58] batch 4400 / 7500, loss: 1.9779997923699293, acc: 0.7670831140135144, f1_avg:0.7768465909090909\n","[TRAIN EPOCH 58] batch 4500 / 7500, loss: 1.9780733597808413, acc: 0.7671942299983994, f1_avg:0.7767777777777778\n","[TRAIN EPOCH 58] batch 4600 / 7500, loss: 1.9783418627148088, acc: 0.7668604268617887, f1_avg:0.7765217391304348\n","[TRAIN EPOCH 58] batch 4700 / 7500, loss: 1.9777243737717893, acc: 0.7676204887016089, f1_avg:0.777127659574468\n","[TRAIN EPOCH 58] batch 4800 / 7500, loss: 1.978386480808258, acc: 0.7669705330838177, f1_avg:0.7764583333333334\n","[TRAIN EPOCH 58] batch 4900 / 7500, loss: 1.9782221695355007, acc: 0.7672065571899791, f1_avg:0.7766326530612245\n","[TRAIN EPOCH 58] batch 5000 / 7500, loss: 1.978218717432022, acc: 0.7672764736652271, f1_avg:0.776625\n","[TRAIN EPOCH 58] batch 5100 / 7500, loss: 1.9783802344051062, acc: 0.7671118556008296, f1_avg:0.7764705882352941\n","[TRAIN EPOCH 58] batch 5200 / 7500, loss: 1.9785506149209462, acc: 0.766998364482743, f1_avg:0.776298076923077\n","[TRAIN EPOCH 58] batch 5300 / 7500, loss: 1.9788209806073387, acc: 0.7667955651528797, f1_avg:0.7760377358490566\n","[TRAIN EPOCH 58] batch 5400 / 7500, loss: 1.979067144813361, acc: 0.7665639012479322, f1_avg:0.7757638888888889\n","[TRAIN EPOCH 58] batch 5500 / 7500, loss: 1.9788895681771366, acc: 0.7667649039092247, f1_avg:0.7759318181818182\n","[TRAIN EPOCH 58] batch 5600 / 7500, loss: 1.9793732058150428, acc: 0.7662489064754713, f1_avg:0.7754241071428571\n","[TRAIN EPOCH 58] batch 5700 / 7500, loss: 1.9792311053945306, acc: 0.7664348070302034, f1_avg:0.7755701754385965\n","[TRAIN EPOCH 58] batch 5800 / 7500, loss: 1.9794897070835376, acc: 0.7662170613151229, f1_avg:0.7753017241379311\n","[TRAIN EPOCH 58] batch 5900 / 7500, loss: 1.9793786686961934, acc: 0.7662889472815325, f1_avg:0.7754025423728813\n","[TRAIN EPOCH 58] batch 6000 / 7500, loss: 1.9794991181691488, acc: 0.7661803616522371, f1_avg:0.7752916666666667\n","[TRAIN EPOCH 58] batch 6100 / 7500, loss: 1.979459549931229, acc: 0.7661890169256974, f1_avg:0.7753483606557378\n","[TRAIN EPOCH 58] batch 6200 / 7500, loss: 1.9796747453366557, acc: 0.7659574736419489, f1_avg:0.7751209677419355\n","[TRAIN EPOCH 58] batch 6300 / 7500, loss: 1.9796365413968526, acc: 0.7660362854737842, f1_avg:0.7751388888888889\n","[TRAIN EPOCH 58] batch 6400 / 7500, loss: 1.9795494313165545, acc: 0.7660927931660341, f1_avg:0.77521484375\n","[TRAIN EPOCH 58] batch 6500 / 7500, loss: 1.9795241460616773, acc: 0.7660807428682411, f1_avg:0.77525\n","[TRAIN EPOCH 58] batch 6600 / 7500, loss: 1.9795789913155817, acc: 0.766012239549169, f1_avg:0.7751893939393939\n","[TRAIN EPOCH 58] batch 6700 / 7500, loss: 1.9797196146623413, acc: 0.7658263188925477, f1_avg:0.7750559701492538\n","[TRAIN EPOCH 58] batch 6800 / 7500, loss: 1.9797443385159268, acc: 0.7657486034186369, f1_avg:0.7750367647058823\n","[TRAIN EPOCH 58] batch 6900 / 7500, loss: 1.9798576773249585, acc: 0.765641825867366, f1_avg:0.774927536231884\n","[TRAIN EPOCH 58] batch 7000 / 7500, loss: 1.9799139224972044, acc: 0.7655044256338859, f1_avg:0.774875\n","[TRAIN EPOCH 58] batch 7100 / 7500, loss: 1.9799271978123087, acc: 0.7656527536430666, f1_avg:0.7748591549295775\n","[TRAIN EPOCH 58] batch 7200 / 7500, loss: 1.979511710587475, acc: 0.7661714653980239, f1_avg:0.7752604166666667\n","[TRAIN EPOCH 58] batch 7300 / 7500, loss: 1.9794208776950837, acc: 0.7662918334519323, f1_avg:0.7753595890410959\n","[TRAIN EPOCH 58] batch 7400 / 7500, loss: 1.9794317221963729, acc: 0.766308160026906, f1_avg:0.7753547297297297\n","[TRAIN EPOCH 58] batch 7500 / 7500, loss: 1.9793714594523113, acc: 0.7663623845598803, f1_avg:0.7754166666666666\n","EPOCH  58 / 60 :  acc: 0.7749206385281384, f1_score: 0.7836333333333333\n","EPOCH  59 / 60 :\n","[TRAIN EPOCH 59] batch 100 / 7500, loss: 1.9787938845157624, acc: 0.7710357142857143, f1_avg:0.77625\n","[TRAIN EPOCH 59] batch 200 / 7500, loss: 1.983984368443489, acc: 0.7655148809523808, f1_avg:0.770625\n","[TRAIN EPOCH 59] batch 300 / 7500, loss: 1.9707282861073812, acc: 0.7769980158730159, f1_avg:0.7841666666666667\n","[TRAIN EPOCH 59] batch 400 / 7500, loss: 1.9713680666685105, acc: 0.7756979166666664, f1_avg:0.7834375\n","[TRAIN EPOCH 59] batch 500 / 7500, loss: 1.9753470859527589, acc: 0.7720416666666667, f1_avg:0.7795\n","[TRAIN EPOCH 59] batch 600 / 7500, loss: 1.975042634208997, acc: 0.7719652777777776, f1_avg:0.78\n","[TRAIN EPOCH 59] batch 700 / 7500, loss: 1.9759221965926035, acc: 0.7701632653061223, f1_avg:0.7792857142857142\n","[TRAIN EPOCH 59] batch 800 / 7500, loss: 1.9720564420521258, acc: 0.7741852678571429, f1_avg:0.783125\n","[TRAIN EPOCH 59] batch 900 / 7500, loss: 1.973628482553694, acc: 0.7720899470899476, f1_avg:0.7815277777777778\n","[TRAIN EPOCH 59] batch 1000 / 7500, loss: 1.9732669773101807, acc: 0.7726690476190481, f1_avg:0.781875\n","[TRAIN EPOCH 59] batch 1100 / 7500, loss: 1.9734091242876919, acc: 0.7723511904761913, f1_avg:0.7817045454545455\n","[TRAIN EPOCH 59] batch 1200 / 7500, loss: 1.9731649865706762, acc: 0.7719642857142868, f1_avg:0.781875\n","[TRAIN EPOCH 59] batch 1300 / 7500, loss: 1.973417359315432, acc: 0.7717261904761918, f1_avg:0.7816346153846154\n","[TRAIN EPOCH 59] batch 1400 / 7500, loss: 1.9727214218888964, acc: 0.7722393707483005, f1_avg:0.7823214285714286\n","[TRAIN EPOCH 59] batch 1500 / 7500, loss: 1.9743479278087617, acc: 0.7703484126984137, f1_avg:0.7806666666666666\n","[TRAIN EPOCH 59] batch 1600 / 7500, loss: 1.9746119213849307, acc: 0.7697358630952386, f1_avg:0.7803125\n","[TRAIN EPOCH 59] batch 1700 / 7500, loss: 1.974611323370653, acc: 0.7696721521942114, f1_avg:0.7802941176470588\n","[TRAIN EPOCH 59] batch 1800 / 7500, loss: 1.9746910967429478, acc: 0.769660604056437, f1_avg:0.7802083333333333\n","[TRAIN EPOCH 59] batch 1900 / 7500, loss: 1.9744058673005356, acc: 0.7703261800334169, f1_avg:0.7804605263157894\n","[TRAIN EPOCH 59] batch 2000 / 7500, loss: 1.9739855659008025, acc: 0.7707369543650792, f1_avg:0.780875\n","[TRAIN EPOCH 59] batch 2100 / 7500, loss: 1.9753446891194297, acc: 0.7694661281179132, f1_avg:0.7795238095238095\n","[TRAIN EPOCH 59] batch 2200 / 7500, loss: 1.976632966724309, acc: 0.7682027867965359, f1_avg:0.7782386363636363\n","[TRAIN EPOCH 59] batch 2300 / 7500, loss: 1.9771244333101357, acc: 0.7677136387163552, f1_avg:0.7777717391304347\n","[TRAIN EPOCH 59] batch 2400 / 7500, loss: 1.976889536778132, acc: 0.7683002232142849, f1_avg:0.77796875\n","[TRAIN EPOCH 59] batch 2500 / 7500, loss: 1.9761865226268769, acc: 0.7690351190476182, f1_avg:0.7787\n","[TRAIN EPOCH 59] batch 2600 / 7500, loss: 1.9772265529174071, acc: 0.7679872939560427, f1_avg:0.7776923076923077\n","[TRAIN EPOCH 59] batch 2700 / 7500, loss: 1.9769888922461758, acc: 0.768270171957671, f1_avg:0.7779166666666667\n","[TRAIN EPOCH 59] batch 2800 / 7500, loss: 1.9770227786472865, acc: 0.7682494685374148, f1_avg:0.7778571428571428\n","[TRAIN EPOCH 59] batch 2900 / 7500, loss: 1.9778104950641764, acc: 0.7673325465243568, f1_avg:0.7770689655172414\n","[TRAIN EPOCH 59] batch 3000 / 7500, loss: 1.9774228440523147, acc: 0.7676595568783067, f1_avg:0.7774583333333334\n","[TRAIN EPOCH 59] batch 3100 / 7500, loss: 1.9768706120214155, acc: 0.768117159498208, f1_avg:0.7780241935483871\n","[TRAIN EPOCH 59] batch 3200 / 7500, loss: 1.9771872586756944, acc: 0.7678973524305553, f1_avg:0.777734375\n","[TRAIN EPOCH 59] batch 3300 / 7500, loss: 1.9771201501109383, acc: 0.7678438251563252, f1_avg:0.7777651515151515\n","[TRAIN EPOCH 59] batch 3400 / 7500, loss: 1.977326649427414, acc: 0.7677724381419232, f1_avg:0.7775367647058824\n","[TRAIN EPOCH 59] batch 3500 / 7500, loss: 1.977854422398976, acc: 0.767408701814059, f1_avg:0.7770357142857143\n","[TRAIN EPOCH 59] batch 3600 / 7500, loss: 1.9776702354682816, acc: 0.7675421902557322, f1_avg:0.7772222222222223\n","[TRAIN EPOCH 59] batch 3700 / 7500, loss: 1.9780308178953223, acc: 0.7672057861432867, f1_avg:0.7768581081081081\n","[TRAIN EPOCH 59] batch 3800 / 7500, loss: 1.9779845304865586, acc: 0.767241097535506, f1_avg:0.7769078947368421\n","[TRAIN EPOCH 59] batch 3900 / 7500, loss: 1.9780421363390408, acc: 0.7672220950345953, f1_avg:0.7768589743589743\n","[TRAIN EPOCH 59] batch 4000 / 7500, loss: 1.977768572717905, acc: 0.7675989831349207, f1_avg:0.777125\n","[TRAIN EPOCH 59] batch 4100 / 7500, loss: 1.977755427505912, acc: 0.767506266937669, f1_avg:0.7771341463414634\n","[TRAIN EPOCH 59] batch 4200 / 7500, loss: 1.977812753603572, acc: 0.7674165013227512, f1_avg:0.7770833333333333\n","[TRAIN EPOCH 59] batch 4300 / 7500, loss: 1.978297285590061, acc: 0.766915951458103, f1_avg:0.7765988372093023\n","[TRAIN EPOCH 59] batch 4400 / 7500, loss: 1.978369746397842, acc: 0.7667626939033195, f1_avg:0.7765340909090909\n","[TRAIN EPOCH 59] batch 4500 / 7500, loss: 1.9785016301208072, acc: 0.7667909832451506, f1_avg:0.7763888888888889\n","[TRAIN EPOCH 59] batch 4600 / 7500, loss: 1.9786090318534686, acc: 0.7666100974810223, f1_avg:0.7762771739130435\n","[TRAIN EPOCH 59] batch 4700 / 7500, loss: 1.9779885071642855, acc: 0.7673596546774747, f1_avg:0.7769148936170213\n","[TRAIN EPOCH 59] batch 4800 / 7500, loss: 1.9785114503651857, acc: 0.766760147982805, f1_avg:0.7763541666666667\n","[TRAIN EPOCH 59] batch 4900 / 7500, loss: 1.978368857028533, acc: 0.7669342606090067, f1_avg:0.7764795918367347\n","[TRAIN EPOCH 59] batch 5000 / 7500, loss: 1.9783328510046005, acc: 0.7670830753968265, f1_avg:0.776525\n","[TRAIN EPOCH 59] batch 5100 / 7500, loss: 1.9784046997276008, acc: 0.7670620720510438, f1_avg:0.7764460784313726\n","[TRAIN EPOCH 59] batch 5200 / 7500, loss: 1.978673915542089, acc: 0.7668109165140427, f1_avg:0.7761778846153846\n","[TRAIN EPOCH 59] batch 5300 / 7500, loss: 1.978986330482195, acc: 0.7665741427073985, f1_avg:0.7758726415094339\n","[TRAIN EPOCH 59] batch 5400 / 7500, loss: 1.9792470531331168, acc: 0.7664283693415647, f1_avg:0.775625\n","[TRAIN EPOCH 59] batch 5500 / 7500, loss: 1.9790736656188965, acc: 0.7665849747474752, f1_avg:0.7757954545454545\n","[TRAIN EPOCH 59] batch 5600 / 7500, loss: 1.9794021824640886, acc: 0.7662455534297057, f1_avg:0.77546875\n","[TRAIN EPOCH 59] batch 5700 / 7500, loss: 1.979338700039345, acc: 0.7663738686995268, f1_avg:0.7755263157894737\n","[TRAIN EPOCH 59] batch 5800 / 7500, loss: 1.979618566714484, acc: 0.7661765701970441, f1_avg:0.7752586206896551\n","[TRAIN EPOCH 59] batch 5900 / 7500, loss: 1.9794583370119838, acc: 0.766266797820822, f1_avg:0.7754025423728813\n","[TRAIN EPOCH 59] batch 6000 / 7500, loss: 1.9794424777030946, acc: 0.7663284226190465, f1_avg:0.7754166666666666\n","[TRAIN EPOCH 59] batch 6100 / 7500, loss: 1.97948257305583, acc: 0.7662335577673676, f1_avg:0.7753688524590164\n","[TRAIN EPOCH 59] batch 6200 / 7500, loss: 1.979681382602261, acc: 0.7660335541474632, f1_avg:0.7751612903225806\n","[TRAIN EPOCH 59] batch 6300 / 7500, loss: 1.9797191610790434, acc: 0.7660873488284179, f1_avg:0.7751388888888889\n","[TRAIN EPOCH 59] batch 6400 / 7500, loss: 1.9796141632646322, acc: 0.7662451791914658, f1_avg:0.775234375\n","[TRAIN EPOCH 59] batch 6500 / 7500, loss: 1.9794822727900285, acc: 0.7663864621489591, f1_avg:0.7753846153846153\n","[TRAIN EPOCH 59] batch 6600 / 7500, loss: 1.9793880651936386, acc: 0.7665139941077406, f1_avg:0.7754924242424243\n","[TRAIN EPOCH 59] batch 6700 / 7500, loss: 1.9795763503971384, acc: 0.7662210820895488, f1_avg:0.7752985074626866\n","[TRAIN EPOCH 59] batch 6800 / 7500, loss: 1.9794115938158596, acc: 0.7663403799019569, f1_avg:0.7754595588235295\n","[TRAIN EPOCH 59] batch 6900 / 7500, loss: 1.9795543926176817, acc: 0.7661592046238744, f1_avg:0.7752898550724637\n","[TRAIN EPOCH 59] batch 7000 / 7500, loss: 1.979571983235223, acc: 0.7660450821995415, f1_avg:0.7752678571428572\n","[TRAIN EPOCH 59] batch 7100 / 7500, loss: 1.9795175900929411, acc: 0.7662474150458255, f1_avg:0.7753169014084507\n","[TRAIN EPOCH 59] batch 7200 / 7500, loss: 1.9790058878891998, acc: 0.7668545111331517, f1_avg:0.7758333333333334\n","[TRAIN EPOCH 59] batch 7300 / 7500, loss: 1.9791783398634768, acc: 0.7667252255925148, f1_avg:0.7756506849315068\n","[TRAIN EPOCH 59] batch 7400 / 7500, loss: 1.9793221329353952, acc: 0.7665995146932593, f1_avg:0.7755067567567567\n","[TRAIN EPOCH 59] batch 7500 / 7500, loss: 1.9791346761067707, acc: 0.7667742989417932, f1_avg:0.7757\n","EPOCH  59 / 60 :  acc: 0.7749196067821067, f1_score: 0.7836333333333333\n","EPOCH  60 / 60 :\n","[TRAIN EPOCH 60] batch 100 / 7500, loss: 1.9779061126708983, acc: 0.7736607142857145, f1_avg:0.7775\n","[TRAIN EPOCH 60] batch 200 / 7500, loss: 1.9847711783647537, acc: 0.7662648809523809, f1_avg:0.77\n","[TRAIN EPOCH 60] batch 300 / 7500, loss: 1.9710799753665924, acc: 0.7769861111111112, f1_avg:0.7833333333333333\n","[TRAIN EPOCH 60] batch 400 / 7500, loss: 1.9698922762274742, acc: 0.776563988095238, f1_avg:0.784375\n","[TRAIN EPOCH 60] batch 500 / 7500, loss: 1.972870157957077, acc: 0.7738761904761906, f1_avg:0.7815\n","[TRAIN EPOCH 60] batch 600 / 7500, loss: 1.9723226362466812, acc: 0.7734692460317459, f1_avg:0.7820833333333334\n","[TRAIN EPOCH 60] batch 700 / 7500, loss: 1.972965191432408, acc: 0.7719336734693877, f1_avg:0.7814285714285715\n","[TRAIN EPOCH 60] batch 800 / 7500, loss: 1.9695158326625823, acc: 0.7759270833333332, f1_avg:0.78484375\n","[TRAIN EPOCH 60] batch 900 / 7500, loss: 1.9723070715533362, acc: 0.7726150793650794, f1_avg:0.7820833333333334\n","[TRAIN EPOCH 60] batch 1000 / 7500, loss: 1.9724620826244355, acc: 0.7728458333333339, f1_avg:0.782\n","[TRAIN EPOCH 60] batch 1100 / 7500, loss: 1.97207036560232, acc: 0.772936688311689, f1_avg:0.7823863636363636\n","[TRAIN EPOCH 60] batch 1200 / 7500, loss: 1.971829128464063, acc: 0.7726021825396835, f1_avg:0.7825\n","[TRAIN EPOCH 60] batch 1300 / 7500, loss: 1.971426040484355, acc: 0.7730178571428585, f1_avg:0.7829807692307692\n","[TRAIN EPOCH 60] batch 1400 / 7500, loss: 1.971363986304828, acc: 0.7729413265306133, f1_avg:0.7830357142857143\n","[TRAIN EPOCH 60] batch 1500 / 7500, loss: 1.9726946493784587, acc: 0.7717230158730168, f1_avg:0.7818333333333334\n","[TRAIN EPOCH 60] batch 1600 / 7500, loss: 1.9735080629587174, acc: 0.7706278521825404, f1_avg:0.781015625\n","[TRAIN EPOCH 60] batch 1700 / 7500, loss: 1.9735686943811528, acc: 0.7705610410831008, f1_avg:0.7810294117647059\n","[TRAIN EPOCH 60] batch 1800 / 7500, loss: 1.9736591267585755, acc: 0.7705186287477959, f1_avg:0.7809027777777777\n","[TRAIN EPOCH 60] batch 1900 / 7500, loss: 1.973375885423861, acc: 0.77115142021721, f1_avg:0.7811184210526316\n","[TRAIN EPOCH 60] batch 2000 / 7500, loss: 1.9728972730636596, acc: 0.7717108134920635, f1_avg:0.7816875\n","[TRAIN EPOCH 60] batch 2100 / 7500, loss: 1.9743948087805794, acc: 0.7703380574451999, f1_avg:0.7802380952380953\n","[TRAIN EPOCH 60] batch 2200 / 7500, loss: 1.9757433369484814, acc: 0.7691146284271279, f1_avg:0.7789772727272727\n","[TRAIN EPOCH 60] batch 2300 / 7500, loss: 1.9762756735345592, acc: 0.768665545203588, f1_avg:0.7784239130434782\n","[TRAIN EPOCH 60] batch 2400 / 7500, loss: 1.976265101581812, acc: 0.7689488260582005, f1_avg:0.7784375\n","[TRAIN EPOCH 60] batch 2500 / 7500, loss: 1.9758706258773804, acc: 0.769292777777777, f1_avg:0.7788\n","[TRAIN EPOCH 60] batch 2600 / 7500, loss: 1.9767115637430779, acc: 0.7684145299145291, f1_avg:0.7779807692307692\n","[TRAIN EPOCH 60] batch 2700 / 7500, loss: 1.976538784901301, acc: 0.7686370517342734, f1_avg:0.7781481481481481\n","[TRAIN EPOCH 60] batch 2800 / 7500, loss: 1.9765576294064522, acc: 0.7686585175736962, f1_avg:0.778125\n","[TRAIN EPOCH 60] batch 2900 / 7500, loss: 1.9775384487776921, acc: 0.7676168582375478, f1_avg:0.7771551724137931\n","[TRAIN EPOCH 60] batch 3000 / 7500, loss: 1.977367685119311, acc: 0.7677159391534387, f1_avg:0.7773333333333333\n","[TRAIN EPOCH 60] batch 3100 / 7500, loss: 1.9769932032400561, acc: 0.768003712237583, f1_avg:0.7777016129032258\n","[TRAIN EPOCH 60] batch 3200 / 7500, loss: 1.9772248379141093, acc: 0.7677991691468254, f1_avg:0.7774609375\n","[TRAIN EPOCH 60] batch 3300 / 7500, loss: 1.9772063480001507, acc: 0.767751142376143, f1_avg:0.7775\n","[TRAIN EPOCH 60] batch 3400 / 7500, loss: 1.977479004334001, acc: 0.7676706720567017, f1_avg:0.7772426470588235\n","[TRAIN EPOCH 60] batch 3500 / 7500, loss: 1.9780884316308158, acc: 0.7671409657802521, f1_avg:0.7766428571428572\n","[TRAIN EPOCH 60] batch 3600 / 7500, loss: 1.9779592049784132, acc: 0.7671811968895313, f1_avg:0.7767361111111111\n","[TRAIN EPOCH 60] batch 3700 / 7500, loss: 1.978293183816446, acc: 0.7668872070122086, f1_avg:0.7764189189189189\n","[TRAIN EPOCH 60] batch 3800 / 7500, loss: 1.9783678261857285, acc: 0.7669263594592557, f1_avg:0.7763486842105263\n","[TRAIN EPOCH 60] batch 3900 / 7500, loss: 1.9784103275873721, acc: 0.7669284003034017, f1_avg:0.7763141025641026\n","[TRAIN EPOCH 60] batch 4000 / 7500, loss: 1.978173644632101, acc: 0.7672926902958166, f1_avg:0.7765625\n","[TRAIN EPOCH 60] batch 4100 / 7500, loss: 1.9781477892398833, acc: 0.7671757954105521, f1_avg:0.7765853658536586\n","[TRAIN EPOCH 60] batch 4200 / 7500, loss: 1.978051617571286, acc: 0.7672539510753807, f1_avg:0.7766964285714286\n","[TRAIN EPOCH 60] batch 4300 / 7500, loss: 1.9786478992950085, acc: 0.7666583442397409, f1_avg:0.7761046511627907\n","[TRAIN EPOCH 60] batch 4400 / 7500, loss: 1.9786068883809176, acc: 0.7666244424767167, f1_avg:0.7761363636363636\n","[TRAIN EPOCH 60] batch 4500 / 7500, loss: 1.978656071000629, acc: 0.7667105659772343, f1_avg:0.7760833333333333\n","[TRAIN EPOCH 60] batch 4600 / 7500, loss: 1.9787668651083241, acc: 0.7665510971516426, f1_avg:0.7759782608695652\n","[TRAIN EPOCH 60] batch 4700 / 7500, loss: 1.978124686124477, acc: 0.7673487688434519, f1_avg:0.7766489361702128\n","[TRAIN EPOCH 60] batch 4800 / 7500, loss: 1.9786012882739306, acc: 0.7668545236592132, f1_avg:0.776171875\n","[TRAIN EPOCH 60] batch 4900 / 7500, loss: 1.9784844389497018, acc: 0.7670523419324462, f1_avg:0.7763010204081633\n","[TRAIN EPOCH 60] batch 5000 / 7500, loss: 1.978263014435768, acc: 0.7672687950937972, f1_avg:0.7765\n","[TRAIN EPOCH 60] batch 5100 / 7500, loss: 1.9784219122867959, acc: 0.7671272034066177, f1_avg:0.7763235294117647\n","[TRAIN EPOCH 60] batch 5200 / 7500, loss: 1.9787036409515601, acc: 0.7668773136585662, f1_avg:0.7760336538461539\n","[TRAIN EPOCH 60] batch 5300 / 7500, loss: 1.9789740064459027, acc: 0.7666179483514411, f1_avg:0.7757547169811321\n","[TRAIN EPOCH 60] batch 5400 / 7500, loss: 1.9792330595740566, acc: 0.766469048955162, f1_avg:0.7755092592592593\n","[TRAIN EPOCH 60] batch 5500 / 7500, loss: 1.9789587073109367, acc: 0.7667211268529467, f1_avg:0.77575\n","[TRAIN EPOCH 60] batch 5600 / 7500, loss: 1.9795577087359768, acc: 0.7661344825809129, f1_avg:0.7751339285714286\n","[TRAIN EPOCH 60] batch 5700 / 7500, loss: 1.9794440934741706, acc: 0.7663143496367194, f1_avg:0.7752412280701755\n","[TRAIN EPOCH 60] batch 5800 / 7500, loss: 1.9796656641672397, acc: 0.7661346003134807, f1_avg:0.775\n","[TRAIN EPOCH 60] batch 5900 / 7500, loss: 1.9794755503686807, acc: 0.7662749743194657, f1_avg:0.7751906779661017\n","[TRAIN EPOCH 60] batch 6000 / 7500, loss: 1.979488996108373, acc: 0.7662455898268393, f1_avg:0.7751666666666667\n","[TRAIN EPOCH 60] batch 6100 / 7500, loss: 1.979592956109125, acc: 0.766102902561918, f1_avg:0.7751024590163934\n","[TRAIN EPOCH 60] batch 6200 / 7500, loss: 1.9797118375762817, acc: 0.7659829632732841, f1_avg:0.775\n","[TRAIN EPOCH 60] batch 6300 / 7500, loss: 1.979658850753118, acc: 0.7661023757988024, f1_avg:0.7750396825396826\n","[TRAIN EPOCH 60] batch 6400 / 7500, loss: 1.979574371315539, acc: 0.7661930070797237, f1_avg:0.7751171875\n","[TRAIN EPOCH 60] batch 6500 / 7500, loss: 1.9794448736080756, acc: 0.7662935175935147, f1_avg:0.77525\n","[TRAIN EPOCH 60] batch 6600 / 7500, loss: 1.9793904635942343, acc: 0.7663770032576818, f1_avg:0.7753030303030303\n","[TRAIN EPOCH 60] batch 6700 / 7500, loss: 1.979581821196115, acc: 0.7661526781676, f1_avg:0.7751305970149254\n","[TRAIN EPOCH 60] batch 6800 / 7500, loss: 1.9795089359669125, acc: 0.7662012907435658, f1_avg:0.7752205882352942\n","[TRAIN EPOCH 60] batch 6900 / 7500, loss: 1.979724665724713, acc: 0.7659720104774407, f1_avg:0.775\n","[TRAIN EPOCH 60] batch 7000 / 7500, loss: 1.9798038338763373, acc: 0.7658238559059936, f1_avg:0.7749107142857142\n","[TRAIN EPOCH 60] batch 7100 / 7500, loss: 1.9797197408575407, acc: 0.7659964407658014, f1_avg:0.775\n","[TRAIN EPOCH 60] batch 7200 / 7500, loss: 1.979266697400146, acc: 0.7665804022366468, f1_avg:0.77546875\n","[TRAIN EPOCH 60] batch 7300 / 7500, loss: 1.9793425880719537, acc: 0.7665479081420811, f1_avg:0.7753938356164384\n","[TRAIN EPOCH 60] batch 7400 / 7500, loss: 1.979403371053773, acc: 0.7664971481221429, f1_avg:0.7753209459459459\n","[TRAIN EPOCH 60] batch 7500 / 7500, loss: 1.979335083468755, acc: 0.7665509956709903, f1_avg:0.7753833333333333\n","EPOCH  60 / 60 :  acc: 0.7749823051948052, f1_score: 0.7836666666666666\n"],"name":"stdout"}]}]}